import torch
from torch_geometric.nn import Node2Vec
import os.path as osp
import torch
import math
import time
from sklearn.metrics.pairwise import cosine_similarity
import pickle

def compare_embeddings(a, b, measure='cosine_similarity'):
    """
        Provided 2 embeddings computes their cosine similarity or their euclidean distance
    """
    if len(a) !=  len(b):
        raise Exception('Shape mismatch')
    
    distance = None

    if measure=='cosine_similarity':
        cosi = torch.nn.CosineSimilarity(dim=0)
        distance = cosi(a,b)

    elif measure=='euclidean_distance':
        cumulative_distance = 0
        for i in range(len(a)):
            x = a[i]
            y = b[i]

            z = (x-y)**2

            cumulative_distance += z
        distance = math.sqrt(cumulative_distance) 

    if distance==None:
        raise Exception('Distance not computed')

    return distance

def train(model, loader, optimizer, device):
    model.train()
    total_loss = 0
    for pos_rw, neg_rw in loader:   
        optimizer.zero_grad()
        loss = model.loss(pos_rw.to(device), neg_rw.to(device))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

class Embeddings:
    def __init__(self, graph=None, directory_name=None):

        if not(graph) and not(directory_name):
            raise Exception('You need to provide one between the directory name and the graph to istantiate an embedding object')

        if directory_name:
            self.load_embedding_tensor(directory_name)
            self.edge_tensor = None
        else:
            self.index_to_token = graph.index_to_token
            self.token_to_index = graph.token_to_index
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            self.edge_tensor = torch.tensor(graph.edges, dtype=torch.long).to(device=device)
            self.embeddings = None

    def __getitem__(self, i):
        if self.embeddings==None:
            raise Exception('Before accessing the embeddings you need to generate them')
        try:
            return self.embeddings[i]
        except IndexError:
            raise Exception(f'Index out of range')
        except:
            raise Exception('Something went wrong')
        
    def generateNode2vecEmbeddings(self, embedding_dim=300, walk_length=60, context_size=60, walks_per_node=20, num_negative_samples=3, p=200, q=1, batch_size=256, num_workers=0, learning_rate=0.01, n_epochs=50, monitor_epochs=1):
        """
            Parameters:
            - edges: a tensor with shape (2, number_of_edges)
            - num_nodes: number of nodes in the graph
            - embedding_dim: the desired number of dimensions for the embeddings
            - walk_length: length of the random walks generated by node2vec
            - context_size: used to 'subsample' the random walks to generate many others with shoreter length
            - walks_per_node: number of random walks generated for every node in  the graph
            - num_negative_samples: number of neative samples
            - p: return parameter, large values increase the explorative behavior of the random walks, small values increase their loaclity
            - q: in-out parameter, small values increase the explorative behavior of the random walks, large values increase their loaclity 
            - batch_size: to define the size of the batch used by the data loader
            - num_workers: I suggest to set it to zero, it can be used for multiprocessing, but not if you are using an interactive shell
            - learning_rate
            - n_epoches
            - monitor_epochs: every "monitor_epochs" epochs the current situation will be printed

            Output: a tensor of shape (number_of_nodes, embedding_size) containing the embeddings of the nodes

        """
        start = time.time()

        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = Node2Vec(self.edge_tensor, embedding_dim=embedding_dim, 
                        walk_length=walk_length,
                        context_size=context_size, walks_per_node=walks_per_node,
                        num_negative_samples=num_negative_samples, 
                        p=p, q=q,
                        sparse=True).to(device)
        
        if not(type(monitor_epochs) == int) or monitor_epochs < 1:
            verbose = False
        t_start_epoch = 0    #debug
        loader = model.loader(batch_size=batch_size, shuffle=True, num_workers=num_workers)
        optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=learning_rate)
        print('Training is starting')
        for epoch in range(1, n_epochs):
            t_start_epoch = time.time()
            loss = train(model, loader, optimizer, device)
            if monitor_epochs != False and (epoch % monitor_epochs) == 0:
                print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, time passed since start: {time.time()-start}s, t_exec last epoch: {time.time()-t_start_epoch}s')
        
        end = time.time()

        print(f'T_exec embedding generation: {end-start}s')

        self.embeddings = model.embedding.weight
    
    def write_w2v_format(self, filename):
        """
            Parameters:
            - embedding: an object of type embedding
            - index_to_token: dictionary which maps the index of the embeddings to the corresponding tokens
            - filname: the output file

            Output: None, a file in the specified path will be created
        """
        w = self.embeddings
        print(f'Write operation started: {w.shape[0]} embeddings will be saved')
        
        start = time.time()
        
        file = open(filename,"w")

        file.write(f'{w.shape[0]} {w.shape[1]}\n')
        for i in range(w.shape[0]):
            file.write(f'{self.index_to_token[i]} ')
            tensor_list = w[i].cpu().detach().numpy().tolist()
            tensor_string = ' '.join(str(element) for element in tensor_list)
            file.write(tensor_string)
            file.write('\n')
        file.close()
        
        end = time.time()

        print(f'T_exec embedding write in w2vec format: {end-start}s')
    
    def save_embedding_tensor(self, directory_name):
        try:
            torch.save(self.embeddings, directory_name+'/embeddings.t')
            f1 = open(directory_name+'/token_to_index.pkl', 'wb')
            f2 = open(directory_name+'/index_to_token.pkl', 'wb')

            pickle.dump(self.token_to_index, f1)
            pickle.dump(self.index_to_token, f2)

            f1.close()
            f2.close()
        except:
            raise Exception('Embedding write operation failed')

    def load_embedding_tensor(self, directory_name):
        try:
            self.embeddings = torch.load(directory_name+'/embeddings.t')
            f1 = open(directory_name+'/token_to_index.pkl', 'rb')
            f2 = open(directory_name+'/index_to_token.pkl', 'rb')

            self.token_to_index = pickle.load(f1)
            self.index_to_token = pickle.load(f2)
            
            f1.close()
            f2.close()
        except:
            raise Exception('Embedding read operation failed')
        

if __name__=="__main__":
    import pandas as pd
    from graph import *
    global_start = time.time()
    df = pd.read_csv("/home/francesco.pugnaloni/GeneralPurposeTableEmbedding/Tests/MSD/Datasets/msd-master.csv").iloc[:100000,:]
    g = Graph()
    print('Generating graph')
    start = time.time()
    g.add_table(df, 'big_table')
    end = time.time()
    print(f'Graph generated, {end-start}s needed')
    g.save("/home/francesco.pugnaloni/GeneralPurposeTableEmbedding/Tests/MSD/Files")
    e = Embeddings(g)
    print('Embedding Generation starts')
    start=time.time()
    e.generateNode2vecEmbeddings()
    end=time.time()
    print(f'Embeddings generated, {end-start}s nedeed')
    e.save_embedding_tensor("/home/francesco.pugnaloni/GeneralPurposeTableEmbedding/Tests/MSD/Files")
    end = time.time()
    print('Finish')
    print(f'Total t_exec: {end-global_start}')
